#+TITLE: Homework 1
#+author: Christoffer Arvidsson
#+setupfile: ~/Dropbox/org/orbit/articles/setup_file.org

* Problem 1.1
** Unconstrained minimum
The optimization problem is
\begin{align}
\min_{x_1,x_2} \quad & f(x_{1}, x_2) = (x_1-1)^2+2(x_2-2)^2, \\
\text{subject to} \quad &  g(x_1x_2) = x_1^2+x_2^2-1 \leq 0.
\end{align}
and should be solved using the penalty method.

The penalty function takes the form
\begin{align*}
p(\mathbf{x};\mu) &= \mu(\sum_{i=1}^m (\max\{g_{i}(\mathbf{x}), 0\})^2) \\
&= \mu(\max\{g(\mathbf{x}), 0\})^2 \\
&= \mu(\max\{x_{1}^{2} + x_{2}^{2} - 1, 0\})^2.
\end{align*}

Thus
\begin{equation}
f_p(x_1,x_2;\mu) =
\begin{cases}
(x_1-1)^2+2(x_2-2)^2 + \mu(x_{1}^{2} + x_{2}^{2} - 1)^2 & \mbox{if $x_{1}^{2} + x_{2}^{2}> 1 $} ,\\
(x_1-1)^2+2(x_2-2)^2 & \mbox{otherwise.}\\
\end{cases}
\end{equation}

Next calculate the gradient
\begin{equation}
\nabla f_p(x_1,x_2;\mu) =
\begin{cases}
\begin{bmatrix}
2(x_1-1) + 4\mu(x_{1}^3 + x_1x_2^{2} - x_1) \\
4(x_2-2) + 4\mu(x_2^{3} + x_{1}^2x_{2} - x_2) \\
\end{bmatrix} & \mbox{if $x_{1}^{2} + x_{2}^{2}> 1 $} ,\\[4mm]
\begin{bmatrix}
2(x_1-1) \\
4(x_2-2) \\
\end{bmatrix} & \mbox{otherwise.} \\
\end{cases}
\end{equation}

For convex functions such as this one, the global minimum can be found simply
by setting the gradient to 0
\begin{align}
\nabla f_{p}(\mathbf{x};\mu = 0) = 0 \Rightarrow f_{p}^{\ast} = (x_1^{\ast}, x_2^{\ast})^{T} = (1,2).
\end{align}

_Answer_: Unconstrained minimum: $(x_1^{\ast}, x_2^{\ast})^{T} = (1,2)$

** Program results
Table [[tbl:results]] shows the results gathered after running the penalty method algorithm with parameters $\eta=0.00001, T=10^{-6}$. Figure [[fig:convergence]]
shows how each component converges as $\mu$ increases.

#+name: tbl:results
#+attr_latex: :booktabs t
#+caption: The components of $\mathbf{x}^{\ast}$ for increasing values of $\mu$.
| $\mu$ |  $x_{1}$       |        $x_{2}$ |
|-------+----------+----------|
|     / |        < |          |
|     1 | 0.433777 | 1.210166 |
|    10 | 0.331354 | 0.995540 |
|   100 | 0.313738 | 0.955252 |
|  1000 | 0.311790 | 0.950732 |
| 10000 | 0.311593 | 0.950274 |

#+name: fig:convergence
#+caption: Components converge as $\mu$ increases.
#+attr_org: :width 800
[[./img/1_1.png]]

* Problem 1.2
** Task a
\begin{equation}
f(x_1,x_2) = 4x_1^2 - x_1x_2 + 4x_2^2 - 6x_2
\end{equation}

Compute the gradient
\begin{equation}
\nabla f(x_1,x_2) =
\begin{bmatrix}
8x_1^2-x_2 \\
-x_1+8x_2-6
\end{bmatrix}
\end{equation}

Then, consider candidates in the interior of the surface by finding stationary points
\begin{align*}
\nabla f(x_{1}, x_{2}) =
\begin{cases}
8x_1^2-x_2 = 0 \\
-x_1+8x_2-6 = 0
\end{cases}
\Rightarrow
& 64x_2 - x_2 = 48 \\
& x_2 = \frac{48}{63}, x_1 = \frac{2}{21}
\end{align*}
which is inside the surface.

Now evaluate the gradient at the corners to check if they are stationary points
\begin{align*}
\nabla f(0,0) &= (0, -6)^{T} &\mbox{No candidate}\\
\nabla f(0,1) &= (1, 2)^{T} &\mbox{No candidate}\\
\nabla f(1,1) &= (7,1)^{T} &\mbox{No candidate}\\
\end{align*}

Next consider the edges of the surface
- Line $(0,0)$ to $(1,1)$ :: Constraint $0 < x_1 < 1, x_2 = x_1$
\begin{align*}
f(x_{1}, x_{1}) &= 7x_1^2 - 6x_1\\
\frac{df}{dx_{1}} &= 14x_1 - 6 \\
\frac{df}{dx_{1}} &= 0 \Rightarrow x_{1} = \frac{3}{7} &
\mbox{Candidate $(\frac{3}{7}, \frac{3}{7})^{T}$}
\end{align*}
- Line $(0,0)$ to $(0,1)$ :: Constraint $x_1 = 0, 0 < x_2 < 1$
\begin{align*}
f(0, x_{2}) &= 4x_2^2 - 6x_2 \\
\frac{df}{dx_{2}} &= 8x_2 - 6 \\
\frac{df}{dx_{2}} &= 0 \Rightarrow x_{2} = \frac{6}{8} &
\mbox{Candidate $(0, \frac{6}{8})^{T}$}
\end{align*}
- Line $(0,1)$ to $(1,1)$ :: Constraint $0 < x_1 < 1, x_2 = 1$
\begin{align*}
f(x_1, 1) &= 4x_1^2 - x_1 - 2\\
\frac{df}{dx_{1}} &= 8x_{1} -1\\
\frac{df}{dx_{1}} &= 0 \Rightarrow x_{1} = \frac{1}{8} &
\mbox{Candidate $(\frac{1}{8}, 1)^{T}$}
\end{align*}

Finally, evaluate all the found candidates
\begin{alignat*}{2}
f\left(\frac{2}{21}, \frac{16}{21}\right) &= -\frac{16}{7}  &\approx -2.28 \quad&\text{Global minimum}\\
f\left(\frac{3}{7}, \frac{3}{7}\right)    &= -\frac{9}{7}   &\approx -1.28 \quad&\text{Local minimum}\\
f\left(0, \frac{6}{8}\right)              &= -\frac{9}{4}   &\approx -2.25 \quad&\text{Local minimum}\\
f\left(\frac{1}{8}, 1\right)              &= -\frac{33}{16} &\approx -2.06 \quad&\text{Local minimum}
\end{alignat*}

_Answer_: Global minimum: $(x_1^{\ast}, x_{2}^{\ast})^{T} = \left(\frac{2}{21}, \frac{16}{21}\right)$

** Task b
The optimization problem is
\begin{align}
\min_{x_1,x_2} \quad & f(x_1, x_2) = 15 + 2x_{1} + 3x_{2} \\
\text{subject to} \quad & h(x_1, x_2) = x_1^2 + x_1x_2 + x_2^2-21 = 0,
\end{align}
and should be solved using the Lagrange multiplier method.

First define
\begin{equation}
L(x_1,x_2,\lambda) = f(x_1,x_2) + \lambda h(x_1,x_2).
\end{equation}

Next find the critical points of $L$ by calculating the gradient and setting it to 0
\begin{align*}
\nabla_{x_1,x_2,\lambda}L(x_1,x_2,\lambda) = 0 \Rightarrow
\begin{cases}
i) & \frac{\partial L}{\partial x_{1}} = 2 + \lambda(2x_{1} + x_{2}) = 0 \\[1mm]
ii) & \frac{\partial L}{\partial x_{2}} = 3 + \lambda(x_{1} + 2x_{2}) = 0 \\[1mm]
iii) & \frac{\partial L}{\partial \lambda} = x_1^2 + x_1x_2 + x_2^2-21 = 0,
\end{cases}
\end{align*}
which results in a three equations, three variable system, which we can solve.

Do $i - 2ii$ and solve for $x_2$
\begin{align*}
2 + \lambda(2x_{1} + x_{2}) - 2(3 + \lambda(x_{1} + 2x_{2})) &= 0 \\
-3\lambda x_{2} &= 4 \\
x_{2} &= -\frac{4}{3\lambda}.
\end{align*}

Then, insert into Equation $ii$ and solve for $x_1$
\begin{align*}
\lambda x_{1} &= -3 - 2\lambda \left(-\frac{4}{3\lambda}\right)\\
x_1 &= -\frac{1}{3\lambda}.
\end{align*}

So
\begin{equation*}
\begin{cases}
x_{1} = -\frac{1}{3\lambda}\\
x_{2} = -\frac{4}{3\lambda}.
\end{cases}
\end{equation*}

Insert into Equation $iii$ and solve for $\lambda$
\begin{align*}
x_1^2 + x_1x_2 + x_2^2-21 &= 0\\
\left(-\frac{1}{3\lambda}\right)^2 + \left(-\frac{1}{3\lambda}\right)\left(-\frac{4}{3\lambda}\right) + \left(-\frac{4}{3\lambda}\right)^2-21 &= 0\\
\frac{1}{9\lambda^2} +\frac{4}{9\lambda^2} +\frac{16}{9\lambda^2} &= 21\\
\frac{21}{9\lambda^2} &= 21 \\
\lambda^{2} &= 1/9 \\
\lambda &= \pm 1/3.
\end{align*}

Finally, calculate $x_{1}^{\ast}, x_{2}^{\ast}$ and evaluate $f(x_{1}^{\ast}, x_2^{\ast})$ for each $\lambda$ to determine type of optimum
\begin{align*}
\lambda_{1} = \frac{1}{3} \Rightarrow &
\begin{cases}
x_{1} = -1\\
x_{2} = -4
\end{cases} &\Rightarrow
f(-1, -4) = 1 \quad &\text{Global minimum}\\
\lambda_{2} = -\frac{1}{3} \Rightarrow &
\begin{cases}
x_{1} = 1\\
x_{2} = 4
\end{cases} &\Rightarrow
f(1, 4) = 29 \quad &\text{Global maximum}\\
\end{align*}

_Answer_: Global minimum: $(x_{1}^{\ast}, x_{2}^{\ast})^{T} = (-1, -4)$

* Problem 1.3
** Task a
Run the algorithm for 10 runs using parameters shown in Table [[tbl:3-1-params]].
The results are shown in Table [[tbl:3-1-results]]. From these results, we can guess that the minimum is $(x_1^{\ast}, x_2^{\ast})^{T} = (3, 0.5)$, where
$g(3,0.5) = 0$. This would explain why the fitness grows to extreme values.

#+name: tbl:3-1-params
#+attr_latex: :booktabs t
#+caption: Parameters used for the 10 runs.
| Parameter                                  | Value |
|--------------------------------------------+-------|
| /                                          |     < |
| Number of runs                             |    10 |
| Population size ($N$)                      |   100 |
| Clamp value $d$                            |     5 |
| Number of genes                            |    50 |
| Tournament size                            |    20 |
| Tour. select probability $p_{\text{tour}}$ |  0.75 |
| Mutation probability $p_{\text{mut}}$      |   0.1 |
| Crossover probability $p_{\text{cross}}$   |   0.8 |
| Number of generations                      |  2000 |

#+name: tbl:3-1-results
#+attr_latex: :booktabs t
#+caption: Results from the 10 runs of the genetic algorighm. In these runs.
| Run |      $x_{1}$ |      $x_{2}$ |      Fitness | $g(x_{1}, x_{2})$ |
|-----+--------------+--------------+--------------+-------------------|
|   / |            < |              |              |                   |
|   1 | 3.0000021458 | 0.4999981970 | 7.905652e+09 |      0.0000000001 |
|   2 | 3.0000009537 | 0.5000002831 | 5.108653e+12 |      0.0000000000 |
|   3 | 3.0000307560 | 0.5000071377 | 6.373061e+09 |      0.0000000002 |
|   4 | 2.9999890327 | 0.4999970049 | 4.760963e+10 |      0.0000000000 |
|   5 | 2.9999988675 | 0.4999996871 | 4.365803e+12 |      0.0000000000 |
|   6 | 2.9999923110 | 0.4999981970 | 1.029817e+11 |      0.0000000000 |
|   7 | 2.9999988675 | 0.4999996871 | 4.365803e+12 |      0.0000000000 |
|   8 | 3.0000307560 | 0.5000077337 | 6.593386e+09 |      0.0000000002 |
|   9 | 2.9999541640 | 0.4999886602 | 2.973439e+09 |      0.0000000003 |
|  10 | 3.0000045300 | 0.5000011772 | 2.983699e+11 |      0.0000000000 |

#+begin_comment
Raw results
    'Fitness: 7.905652e+09, x(1): 3.0000021458, x(2): 0.4999981970, g: 0.0000000001'
    'Fitness: 5.108653e+12, x(1): 3.0000009537, x(2): 0.5000002831, g: 0.0000000000'
    'Fitness: 6.373061e+09, x(1): 3.0000307560, x(2): 0.5000071377, g: 0.0000000002'
    'Fitness: 4.760963e+10, x(1): 2.9999890327, x(2): 0.4999970049, g: 0.0000000000'
    'Fitness: 4.365803e+12, x(1): 2.9999988675, x(2): 0.4999996871, g: 0.0000000000'
    'Fitness: 1.029817e+11, x(1): 2.9999923110, x(2): 0.4999981970, g: 0.0000000000'
    'Fitness: 4.365803e+12, x(1): 2.9999988675, x(2): 0.4999996871, g: 0.0000000000'
    'Fitness: 6.593386e+09, x(1): 3.0000307560, x(2): 0.5000077337, g: 0.0000000002'
    'Fitness: 2.973439e+09, x(1): 2.9999541640, x(2): 0.4999886602, g: 0.0000000003'
    'Fitness: 2.983699e+11, x(1): 3.0000045300, x(2): 0.5000011772, g: 0.0000000000'
#+end_comment

** Task b
Search the parameter space $p_{\text{mut}} \in [0, 0.11]$ linearly, testing 12 individual values using the
supplied parameters in the =RunBatch.m= script. Table [[tbl:3-2-results]] shows the
statistics depending on chosen $p_{mut}$.

#+name: tbl:3-2-results
#+attr_latex: :booktabs t
#+caption: Statistics gathered from each batch run over $p_{\text{mut}}$.
| $p_{\text{mut}}$ |      median fitness |         average fitness |              std fitness |
|------------------+---------------------+-------------------------+--------------------------|
|                / |                   < |                         |                          |
|             0.00 |       80.4420292199 |        18245.8509068197 |        151884.5287435984 |
|             0.01 |     7026.3063914864 | 740568923589.3103027344 | 5129419207427.0429687500 |
|             0.02 | 84076737.2461014539 |   1937402622.1958365440 |    7793681319.5859441757 |
|             0.03 |  6119044.0555177480 |    271833642.4884516001 |    1906217629.7886574268 |
|             0.04 |   789530.0460310053 |      8744419.7588695511 |      26544219.2598277889 |
|             0.05 |   352362.6655200199 |      5861489.5277889585 |      38684866.2370801568 |
|             0.06 |    89871.1372779460 |      2074848.7576343585 |      11327716.2079971209 |
|             0.07 |    47459.2479326281 |      7787672.8403089587 |      73633148.5249726474 |
|             0.08 |    24739.3041637269 |      2630166.3150149002 |      10913466.5406210143 |
|             0.09 |    36757.8096885269 |       277846.8759837335 |        981534.3423755619 |
|             0.10 |    10743.3395130475 |      1307551.9555330784 |       8340694.9576941608 |
|             0.11 |    19028.1866316668 |       178707.9593138866 |        552357.0243210060 |

#+begin_comment
Raw results
    'PMut: 0.0000000000, Median: 80.4420292199,       Average: 18245.8509068197,        STD: 151884.5287435984'
    'PMut: 0.0100000000, Median: 7026.3063914864,     Average: 740568923589.3103027344, STD: 5129419207427.0429687500'
    'PMut: 0.0200000000, Median: 84076737.2461014539, Average: 1937402622.1958365440,   STD: 7793681319.5859441757'
    'PMut: 0.0300000000, Median: 6119044.0555177480,  Average: 271833642.4884516001,    STD: 1906217629.7886574268'
    'PMut: 0.0400000000, Median: 789530.0460310053,   Average: 8744419.7588695511,      STD: 26544219.2598277889'
    'PMut: 0.0500000000, Median: 352362.6655200199,   Average: 5861489.5277889585,      STD: 38684866.2370801568'
    'PMut: 0.0600000000, Median: 89871.1372779460,    Average: 2074848.7576343585,      STD: 11327716.2079971209'
    'PMut: 0.0700000000, Median: 47459.2479326281,    Average: 7787672.8403089587,      STD: 73633148.5249726474'
    'PMut: 0.0800000000, Median: 24739.3041637269,    Average: 2630166.3150149002,      STD: 10913466.5406210143'
    'PMut: 0.0900000000, Median: 36757.8096885269,    Average: 277846.8759837335,       STD: 981534.3423755619'
    'PMut: 0.1000000000, Median: 10743.3395130475,    Average: 1307551.9555330784,      STD: 8340694.9576941608'
    'PMut: 0.1100000000, Median: 19028.1866316668,    Average: 178707.9593138866,       STD: 552357.0243210060'
#+end_comment

Figure [[fig:3-2-fitness]] shows the median fitness achieved for each
$p_{\text{mut}}$. There is a spike at $p_{\text{mut}} = 0.02$, indicating that
it is an optimal choice in mutation probability, as described in the lectures.
This value is precisely $1/m$, where $m$ is the length of a chromosome. This
means that on average, there will be one mutation of each chromosome, which
supplies the algorithm with new genetic material.

Without mutations the median fitness is low, due to there being no room for
exploring better solutions than whatever the to the initial chromosomes.

#+name: fig:3-2-fitness
#+caption: Logarithmic plot of the median fitness over the mutation probability.
#+attr_org: :width 800
[[file:~/Dropbox/org/orbit/articles/project/stocopt_algorithms/homework1/task3/img/1_3.png]]

** Task c
Task a showed that the minimum seemed to converge to $(x_1^{\ast},
x_2^{\ast})^{T} = (3, 0.5)$. To check if this is a stationary point, first
compute the gradient using the chain rule where applicable
\begin{equation}
\nabla g(x_1,x_2) =
\begin{bmatrix}
2(1.5 - x_1 + x_1x_2)(x_2-1) + 2(2.25 - x_1 + x_1x_2^2)(x_2^2-1) + \\ + 2(2.625 - x_1 + x_1x_2^3)^3(x_2^3-1) \\
2(1.5-x_1+x_1x_2)x_1 + 2(2.25 - x_1 + x_1x_2^2)(2x_1x_2) + \\ + 2(2.625 - x_1 + x_1x_2^3)(3x_1x_2^2)
\end{bmatrix}.
\end{equation}

Then, evaluate the gradient at the found solution
\begin{equation*}
\nabla g(3,0.5) =
\begin{bmatrix}
0+ 0+0\\
0+ 0+0\\
\end{bmatrix}=
(0,0)^T,
\end{equation*}
and note that $(3, 0.5)^T$ is a stationary point since the gradient evaluates to the 0 vector.
