#+TITLE: Homework 1
#+author: Christoffer Arvidsson
#+setupfile: ~/Dropbox/org/orbit/articles/setup_file.org

* TODO add civic registration number
* Problem 1.1
** Unconstrained minimum
The optimization problem is
\begin{align}
\min_{x_1,x_2} \quad & f(x_{1}, x_2) = (x_1-1)^2+2(x_2-2)^2, \\
\text{subject to} \quad &  g(x_1x_2) = x_1^2+x_2^2-1 \leq 0.
\end{align}
and should be solved using the penalty method.

The penalty function takes the form
\begin{align*}
p(\mathbf{x};\mu) &= \mu(\sum_{i=1}^m (\max\{g_{i}(\mathbf{x}), 0\})^2) \\
&= \mu(\max\{g(\mathbf{x}), 0\})^2 \\
&= \mu(\max\{x_{1}^{2} + x_{2}^{2} - 1, 0\})^2.
\end{align*}

Thus
\begin{equation}
f_p(x_1,x_2;\mu) =
\begin{cases}
(x_1-1)^2+2(x_2-2)^2 + \mu(x_{1}^{2} + x_{2}^{2} - 1)^2 & \mbox{if $x_{1}^{2} + x_{2}^{2}> 1 $} ,\\
(x_1-1)^2+2(x_2-2)^2 & \mbox{otherwise.}\\
\end{cases}
\end{equation}

Next calculate the gradient
\begin{equation}
\nabla f_p(x_1,x_2;\mu) =
\begin{cases}
\begin{bmatrix}
2(x_1-1) + 4\mu(x_{1}^3 + x_1x_2^{2} - x_1) \\
4(x_2-2) + 4\mu(x_2^{3} + x_{1}^2x_{2} - x_2) \\
\end{bmatrix} & \mbox{if $x_{1}^{2} + x_{2}^{2}> 1 $} ,\\[4mm]
\begin{bmatrix}
2(x_1-1) \\
4(x_2-2) \\
\end{bmatrix} & \mbox{otherwise.} \\
\end{cases}
\end{equation}

For convex functions such as this one, the global minimum can be found simply
by setting the gradient to 0
\begin{align}
\nabla f_{p}(\mathbf{x};\mu = 0) = 0 \Rightarrow f_{p}^{\ast} = (x_1^{\ast}, x_2^{\ast})^{T} = (1,2).
\end{align}

_Answer_: Unconstrained minimum: $(x_1^{\ast}, x_2^{\ast})^{T} = (1,2)$

** Program results
Table [[tbl:results]] shows the results gathered after running the penalty method algorithm with parameters $\eta=0.00001, T=10^{-6}$. Figure [[fig:convergence]]
shows how each component converges as $\mu$ increases.

#+name: tbl:results
#+attr_latex: :booktabs t
#+caption: The components of $\mathbf{x}^{\ast}$ for increasing values of $\mu$.
| $\mu$ |  $x_{1}$       |        $x_{2}$ |
|-------+----------+----------|
|     / |        < |          |
|     1 | 0.433777 | 1.210166 |
|    10 | 0.331354 | 0.995540 |
|   100 | 0.313738 | 0.955252 |
|  1000 | 0.311790 | 0.950732 |
| 10000 | 0.311593 | 0.950274 |

#+name: fig:convergence
#+caption: Components converge as $\mu$ increases.
#+attr_org: :width 800
[[./img/1_1.png]]

* Problem 1.2
** Task a
\begin{equation}
f(x_1,x_2) = 4x_1^2 - x_1x_2 + 4x_2^2 - 6x_2
\end{equation}

Compute the gradient
\begin{equation}
\nabla f(x_1,x_2) =
\begin{bmatrix}
8x_1^2-x_2 \\
-x_1+8x_2-6
\end{bmatrix}
\end{equation}

Then, consider candidates in the interior of the surface by finding stationary points
\begin{align*}
\nabla f(x_{1}, x_{2}) =
\begin{cases}
8x_1^2-x_2 = 0 \\
-x_1+8x_2-6 = 0
\end{cases}
\Rightarrow
& 64x_2 - x_2 = 48 \\
& x_2 = \frac{48}{63}, x_1 = \frac{2}{21}
\end{align*}
which is inside the surface.

Now evaluate the gradient at the corners to check if they are stationary points
\begin{align*}
\nabla f(0,0) &= (0, -6)^{T} &\mbox{No candidate}\\
\nabla f(0,1) &= (1, 2)^{T} &\mbox{No candidate}\\
\nabla f(1,1) &= (7,1)^{T} &\mbox{No candidate}\\
\end{align*}

Next consider the edges of the surface
- Line $(0,0)$ to $(1,1)$ :: Constraint $0 < x_1 < 1, x_2 = x_1$
\begin{align*}
f(x_{1}, x_{1}) &= 7x_1^2 - 6x_1\\
\frac{df}{dx_{1}} &= 14x_1 - 6 \\
\frac{df}{dx_{1}} &= 0 \Rightarrow x_{1} = \frac{3}{7} &
\mbox{Candidate $(\frac{3}{7}, \frac{3}{7})^{T}$}
\end{align*}
- Line $(0,0)$ to $(0,1)$ :: Constraint $x_1 = 0, 0 < x_2 < 1$
\begin{align*}
f(0, x_{2}) &= 4x_2^2 - 6x_2 \\
\frac{df}{dx_{2}} &= 8x_2 - 6 \\
\frac{df}{dx_{2}} &= 0 \Rightarrow x_{2} = \frac{6}{8} &
\mbox{Candidate $(0, \frac{6}{8})^{T}$}
\end{align*}
- Line $(0,1)$ to $(1,1)$ :: Constraint $0 < x_1 < 1, x_2 = 1$
\begin{align*}
f(x_1, 1) &= 4x_1^2 - x_1 - 2\\
\frac{df}{dx_{1}} &= 8x_{1} -1\\
\frac{df}{dx_{1}} &= 0 \Rightarrow x_{1} = \frac{1}{8} &
\mbox{Candidate $(\frac{1}{8}, 1)^{T}$}
\end{align*}

Finally, evaluate all the found candidates
\begin{alignat*}{2}
f\left(\frac{2}{21}, \frac{16}{21}\right) &= -\frac{16}{7}  &\approx -2.28 \quad&\text{Global minimum}\\
f\left(\frac{3}{7}, \frac{3}{7}\right)    &= -\frac{9}{7}   &\approx -1.28 \quad&\text{Local minimum}\\
f\left(0, \frac{6}{8}\right)              &= -\frac{9}{4}   &\approx -2.25 \quad&\text{Local minimum}\\
f\left(\frac{1}{8}, 1\right)              &= -\frac{33}{16} &\approx -2.06 \quad&\text{Local minimum}
\end{alignat*}

_Answer_: Global minimum: $(x_1^{\ast}, x_{2}^{\ast})^{T} = \left(\frac{2}{21}, \frac{16}{21}\right)$

** Task b
The optimization problem is
\begin{align}
\min_{x_1,x_2} \quad & f(x_1, x_2) = 15 + 2x_{1} + 3x_{2} \\
\text{subject to} \quad & h(x_1, x_2) = x_1^2 + x_1x_2 + x_2^2-21 = 0,
\end{align}
and should be solved using the Lagrange multiplier method.

First define
\begin{equation}
L(x_1,x_2,\lambda) = f(x_1,x_2) + \lambda h(x_1,x_2).
\end{equation}

Next find the critical points of $L$ by calculating the gradient and setting it to 0
\begin{align*}
\nabla_{x_1,x_2,\lambda}L(x_1,x_2,\lambda) = 0 \Rightarrow
\begin{cases}
i) & \frac{\partial L}{\partial x_{1}} = 2 + \lambda(2x_{1} + x_{2}) = 0 \\[1mm]
ii) & \frac{\partial L}{\partial x_{2}} = 3 + \lambda(x_{1} + 2x_{2}) = 0 \\[1mm]
iii) & \frac{\partial L}{\partial \lambda} = x_1^2 + x_1x_2 + x_2^2-21 = 0,
\end{cases}
\end{align*}
which results in a three equations, three variable system, which we can solve.

Do $i - 2ii$ and solve for $x_2$
\begin{align*}
2 + \lambda(2x_{1} + x_{2}) - 2(3 + \lambda(x_{1} + 2x_{2})) &= 0 \\
-3\lambda x_{2} &= 4 \\
x_{2} &= -\frac{4}{3\lambda}.
\end{align*}

Then, insert into Equation $ii$ and solve for $x_1$
\begin{align*}
\lambda x_{1} &= -3 - 2\lambda \left(-\frac{4}{3\lambda}\right)\\
x_1 &= -\frac{1}{3\lambda}.
\end{align*}

So
\begin{equation*}
\begin{cases}
x_{1} = -\frac{1}{3\lambda}\\
x_{2} = -\frac{4}{3\lambda}.
\end{cases}
\end{equation*}

Insert into Equation $iii$ and solve for $\lambda$
\begin{align*}
x_1^2 + x_1x_2 + x_2^2-21 &= 0\\
\left(-\frac{1}{3\lambda}\right)^2 + \left(-\frac{1}{3\lambda}\right)\left(-\frac{4}{3\lambda}\right) + \left(-\frac{4}{3\lambda}\right)^2-21 &= 0\\
\frac{1}{9\lambda^2} +\frac{4}{9\lambda^2} +\frac{16}{9\lambda^2} &= 21\\
\frac{21}{9\lambda^2} &= 21 \\
\lambda^{2} &= 1/9 \\
\lambda &= \pm 1/3.
\end{align*}

Finally, calculate $x_{1}^{\ast}, x_{2}^{\ast}$ and evaluate $f(x_{1}^{\ast}, x_2^{\ast})$ for each $\lambda$ to determine type of optimum
\begin{align*}
\lambda_{1} = \frac{1}{3} \Rightarrow &
\begin{cases}
x_{1} = -1\\
x_{2} = -4
\end{cases} &\Rightarrow
f(-1, -4) = 1 \quad &\text{Global minimum}\\
\lambda_{2} = -\frac{1}{3} \Rightarrow &
\begin{cases}
x_{1} = 1\\
x_{2} = 4
\end{cases} &\Rightarrow
f(1, 4) = 29 \quad &\text{Global maximum}\\
\end{align*}

_Answer_: Global minimum: $(x_{1}^{\ast}, x_{2}^{\ast})^{T} = (-1, -4)$

* Problem 1.3
Run the algorithm for 10 runs using parameters shown in Table [[tbl:3-1-params]].
The results are shown in Table [[tbl:3-1-results]]. From these results, we can guess that the minimum is $(x_1^{\ast}, x_2^{\ast})^{T} = (3, 0.5)$, where
$g(3,0.5) = 0$. This would explain why the fitness grows to extreme values.

#+name: tbl:3-1-params
#+attr_latex: :booktabs t
#+caption: Parameters used for the 10 runs.
| Parameter                                  | Value |
|--------------------------------------------+-------|
| /                                          |     < |
| Number of runs                             |    10 |
| Population size ($N$)                      |   100 |
| Clamp value $d$                            |     5 |
| Number of genes                            |    50 |
| Tournament size                            |    20 |
| Tour. select probability $p_{\text{tour}}$ |  0.75 |
| Mutation probability $p_{\text{mut}}$      |  0.02 |
| Crossover probability $p_{\text{cross}}$   |   0.8 |
| Number of generations                      |  2000 |

#+name: tbl:3-1-results
#+attr_latex: :booktabs t
#+caption: Results from the 10 runs of the genetic algorighm. In these runs.
| Run |      $x_{1}$ |      $x_{2}$ |      Fitness | $g(x_{1}, x_{2})$ |
|-----+--------------+--------------+--------------+-------------------|
|   / |            < |              |              |                   |
|   1 | 2.8906249371 | 0.4687498650 | 4.290043e+02 |      0.0023309790 |
|   2 | 2.9687499395 | 0.4921533314 | 6.178123e+03 |      0.0001618615 |
|   3 | 3.7500002608 | 0.6384276044 | 2.281390e+01 |      0.0438329208 |
|   4 | 3.1250002421 | 0.5273436167 | 4.342576e+02 |      0.0023027805 |
|   5 | 2.9589846122 | 0.4882811155 | 3.095889e+03 |      0.0003230090 |
|   6 | 2.1250154711 | -0.000000149 | 1.523809e+00 |      0.6562503965 |
|   7 | 2.9929515121 | 0.4980467408 | 1.119665e+05 |      0.0000089312 |
|   8 | 2.9980468153 | 0.4995156973 | 1.634116e+06 |      0.0000006120 |
|   9 | 3.0006643534 | 0.5001832694 | 1.272922e+07 |      0.0000000786 |
|  10 | 2.4999999255 | 0.3500364825 | 1.375514e+01 |      0.0727000875 |

#+begin_comment
Raw results
    'Fitness: 4.290043e+02, x(1): 2.8906249371, x(2): 0.4687498650, g: 0.0023309790'
    'Fitness: 6.178123e+03, x(1): 2.9687499395, x(2): 0.4921533314, g: 0.0001618615'
    'Fitness: 2.281390e+01, x(1): 3.7500002608, x(2): 0.6384276044, g: 0.0438329208'
    'Fitness: 4.342576e+02, x(1): 3.1250002421, x(2): 0.5273436167, g: 0.0023027805'
    'Fitness: 3.095889e+03, x(1): 2.9589846122, x(2): 0.4882811155, g: 0.0003230090'
    'Fitness: 1.523809e+00, x(1): 2.1250154711, x(2): -0.0000001490, g: 0.6562503965'
    'Fitness: 1.119665e+05, x(1): 2.9929515121, x(2): 0.4980467408, g: 0.0000089312'
    'Fitness: 1.634116e+06, x(1): 2.9980468153, x(2): 0.4995156973, g: 0.0000006120'
    'Fitness: 1.272922e+07, x(1): 3.0006643534, x(2): 0.5001832694, g: 0.0000000786'
    'Fitness: 1.375514e+01, x(1): 2.4999999255, x(2): 0.3500364825, g: 0.0727000875'
#+end_comment
