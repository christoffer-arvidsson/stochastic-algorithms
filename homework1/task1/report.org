#+TITLE: Homework 1
#+author: Christoffer Arvidsson
#+setupfile: ~/Dropbox/org/orbit/articles/setup_file.org

* TODO add civic registration number
* Problem 1.1
** Unconstrained minimum
The optimization problem is
\begin{align}
\min_{x_1,x_2} \quad & f(x_{1}, x_2) = (x_1-1)^2+2(x_2-2)^2, \\
\text{subject to} \quad &  g(x_1x_2) = x_1^2+x_2^2-1 \leq 0.
\end{align}
and should be solved using the penalty method.

The penalty function takes the form
\begin{align*}
p(\mathbf{x};\mu) &= \mu(\sum_{i=1}^m (\max\{g_{i}(\mathbf{x}), 0\})^2) \\
&= \mu(\max\{g(\mathbf{x}), 0\})^2 \\
&= \mu(\max\{x_{1}^{2} + x_{2}^{2} - 1, 0\})^2.
\end{align*}

Thus
\begin{equation}
f_p(x_1,x_2;\mu) =
\begin{cases}
(x_1-1)^2+2(x_2-2)^2 + \mu(x_{1}^{2} + x_{2}^{2} - 1)^2 & \mbox{if $x_{1}^{2} + x_{2}^{2}> 1 $} ,\\
(x_1-1)^2+2(x_2-2)^2 & \mbox{otherwise.}\\
\end{cases}
\end{equation}

Next calculate the gradient
\begin{equation}
\nabla f_p(x_1,x_2;\mu) =
\begin{cases}
\begin{bmatrix}
2(x_1-1) + 4\mu(x_{1}^3 + x_1x_2^{2} - x_1) \\
4(x_2-2) + 4\mu(x_2^{3} + x_{1}^2x_{2} - x_2) \\
\end{bmatrix} & \mbox{if $x_{1}^{2} + x_{2}^{2}> 1 $} ,\\[4mm]
\begin{bmatrix}
2(x_1-1) \\
4(x_2-2) \\
\end{bmatrix} & \mbox{otherwise.} \\
\end{cases}
\end{equation}

For convex functions such as this one, the global minimum can be found simply
by setting the gradient to 0
\begin{align}
\nabla f_{p}(\mathbf{x};\mu = 0) = 0 \Rightarrow f_{p}^{\ast} = (x_1^{\ast}, x_2^{\ast})^{T} = (1,2).
\end{align}

_Answer_: Unconstrained minimum: $(x_1^{\ast}, x_2^{\ast})^{T} = (1,2)$

** Program results
Table [[tbl:results]] shows the results gathered after running the penalty method algorithm with parameters $\eta=0.00001, T=10^{-6}$. Figure [[fig:convergence]]
shows how each component converges as $\mu$ increases.

#+name: tbl:results
#+attr_latex: :booktabs t
#+caption: The components of $\mathbf{x}^{\ast}$ for increasing values of $\mu$.
| $\mu$ |  $x_{1}$ |  $x_{2}$ |
|-------+----------+----------|
|     / |        < |          |
|     1 | 0.433777 | 1.210166 |
|    10 | 0.331354 | 0.995540 |
|   100 | 0.313738 | 0.955252 |
|  1000 | 0.311790 | 0.950732 |
| 10000 | 0.311593 | 0.950274 |

#+name: fig:convergence
#+caption: Components converge as $\mu$ increases.
#+attr_org: :width 800
[[./img/1_1.png]]

* Problem 1.2
** Task a
\begin{equation}
f(x_1,x_2) = 4x_1^2 - x_1x_2 + 4x_2^2 - 6x_2
\end{equation}

Compute the gradient
\begin{equation}
\nabla f(x_1,x_2) =
\begin{bmatrix}
8x_1^2-x_2 \\
-x_1+8x_2-6
\end{bmatrix}
\end{equation}

Then, consider candidates in the interior of the surface by finding stationary points
\begin{align*}
\nabla f(x_{1}, x_{2}) =
\begin{cases}
8x_1^2-x_2 = 0 \\
-x_1+8x_2-6 = 0
\end{cases}
\Rightarrow
& 64x_2 - x_2 = 48 \\
& x_2 = \frac{48}{63}, x_1 = \frac{2}{21}
\end{align*}
which is inside the surface.

Now evaluate the gradient at the corners to check if they are stationary points
\begin{align*}
\nabla f(0,0) &= (0, -6)^{T} &\mbox{No candidate}\\
\nabla f(0,1) &= (1, 2)^{T} &\mbox{No candidate}\\
\nabla f(1,1) &= (7,1)^{T} &\mbox{No candidate}\\
\end{align*}

Next consider the edges of the surface
- Line $(0,0)$ to $(1,1)$ :: Constraint $0 < x_1 < 1, x_2 = x_1$
\begin{align*}
f(x_{1}, x_{1}) &= 7x_1^2 - 6x_1\\
\frac{df}{dx_{1}} &= 14x_1 - 6 \\
\frac{df}{dx_{1}} &= 0 \Rightarrow x_{1} = \frac{3}{7} &
\mbox{Candidate $(\frac{3}{7}, \frac{3}{7})^{T}$}
\end{align*}
- Line $(0,0)$ to $(0,1)$ :: Constraint $x_1 = 0, 0 < x_2 < 1$
\begin{align*}
f(0, x_{2}) &= 4x_2^2 - 6x_2 \\
\frac{df}{dx_{2}} &= 8x_2 - 6 \\
\frac{df}{dx_{2}} &= 0 \Rightarrow x_{2} = \frac{6}{8} &
\mbox{Candidate $(0, \frac{6}{8})^{T}$}
\end{align*}
- Line $(0,1)$ to $(1,1)$ :: Constraint $0 < x_1 < 1, x_2 = 1$
\begin{align*}
f(x_1, 1) &= 4x_1^2 - x_1 - 2\\
\frac{df}{dx_{1}} &= 8x_{1} -1\\
\frac{df}{dx_{1}} &= 0 \Rightarrow x_{1} = \frac{1}{8} &
\mbox{Candidate $(\frac{1}{8}, 1)^{T}$}
\end{align*}

Finally, evaluate all the found candidates
\begin{alignat*}{2}
f\left(\frac{2}{21}, \frac{16}{21}\right) &= -\frac{16}{7}  &\approx -2.28 \quad&\text{Global minimum}\\
f\left(\frac{3}{7}, \frac{3}{7}\right)    &= -\frac{9}{7}   &\approx -1.28 \quad&\text{Local minimum}\\
f\left(0, \frac{6}{8}\right)              &= -\frac{9}{4}   &\approx -2.25 \quad&\text{Local minimum}\\
f\left(\frac{1}{8}, 1\right)              &= -\frac{33}{16} &\approx -2.06 \quad&\text{Local minimum}
\end{alignat*}

_Answer_: Global minimum: $(x_1^{\ast}, x_{2}^{\ast})^{T} = \left(\frac{2}{21}, \frac{16}{21}\right)$

** Task b
The optimization problem is
\begin{align}
\min_{x_1,x_2} \quad & f(x_1, x_2) = 15 + 2x_{1} + 3x_{2} \\
\text{subject to} \quad & h(x_1, x_2) = x_1^2 + x_1x_2 + x_2^2-21 = 0,
\end{align}
and should be solved using the Lagrange multiplier method.

First define
\begin{equation}
L(x_1,x_2,\lambda) = f(x_1,x_2) + \lambda h(x_1,x_2).
\end{equation}

Next find the critical points of $L$ by calculating the gradient and setting it to 0
\begin{align*}
\nabla_{x_1,x_2,\lambda}L(x_1,x_2,\lambda) = 0 \Rightarrow
\begin{cases}
i) & \frac{\partial L}{\partial x_{1}} = 2 + \lambda(2x_{1} + x_{2}) = 0 \\[1mm]
ii) & \frac{\partial L}{\partial x_{2}} = 3 + \lambda(x_{1} + 2x_{2}) = 0 \\[1mm]
iii) & \frac{\partial L}{\partial \lambda} = x_1^2 + x_1x_2 + x_2^2-21 = 0,
\end{cases}
\end{align*}
which results in a three equations, three variable system, which we can solve.

Do $i - 2ii$ and solve for $x_2$
\begin{align*}
2 + \lambda(2x_{1} + x_{2}) - 2(3 + \lambda(x_{1} + 2x_{2})) &= 0 \\
-3\lambda x_{2} &= 4 \\
x_{2} &= -\frac{4}{3\lambda}.
\end{align*}

Then, insert into Equation $ii$ and solve for $x_1$
\begin{align*}
\lambda x_{1} &= -3 - 2\lambda \left(-\frac{4}{3\lambda}\right)\\
x_1 &= -\frac{1}{3\lambda}.
\end{align*}

So
\begin{equation*}
\begin{cases}
x_{1} = -\frac{1}{3\lambda}\\
x_{2} = -\frac{4}{3\lambda}.
\end{cases}
\end{equation*}

Insert into Equation $iii$ and solve for $\lambda$
\begin{align*}
x_1^2 + x_1x_2 + x_2^2-21 &= 0\\
\left(-\frac{1}{3\lambda}\right)^2 + \left(-\frac{1}{3\lambda}\right)\left(-\frac{4}{3\lambda}\right) + \left(-\frac{4}{3\lambda}\right)^2-21 &= 0\\
\frac{1}{9\lambda^2} +\frac{4}{9\lambda^2} +\frac{16}{9\lambda^2} &= 21\\
\frac{21}{9\lambda^2} &= 21 \\
\lambda^{2} &= 1/9 \\
\lambda &= \pm 1/3.
\end{align*}

Finally, calculate $x_{1}^{\ast}, x_{2}^{\ast}$ and evaluate $f(x_{1}^{\ast}, x_2^{\ast})$ for each $\lambda$ to determine type of optimum
\begin{align*}
\lambda_{1} = \frac{1}{3} \Rightarrow &
\begin{cases}
x_{1} = -1\\
x_{2} = -4
\end{cases} &\Rightarrow
f(-1, -4) = 1 \quad &\text{Global minimum}\\
\lambda_{2} = -\frac{1}{3} \Rightarrow &
\begin{cases}
x_{1} = 1\\
x_{2} = 4
\end{cases} &\Rightarrow
f(1, 4) = 29 \quad &\text{Global maximum}\\
\end{align*}

_Answer_: Global minimum: $(x_{1}^{\ast}, x_{2}^{\ast})^{T} = (-1, -4)$
